version: '3.8'

services:
  postgres:
    image: postgres:15
    container_name: airflow_postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:latest
    container_name: airflow_redis
    restart: always
    ports:
      - "6379:6379"

  airflow-webserver:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile
      args:
        AIRFLOW_UID: ${LOCAL_UID:-1000}
        AIRFLOW_GID: ${LOCAL_GID:-0}
    user: "${LOCAL_UID:-1000}:0"
    container_name: airflow_webserver
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}  # for test environment; should be changed in production.
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=60
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
      - AIRFLOW_UID=1000
      - AIRFLOW_CONN_MY_SNOWFLAKE_CONN=snowflake://${DBT_USER}:${DBT_PASSWORD}@${DBT_ACCOUNT}/${DBT_DATABASE}/${DBT_SCHEMA}?warehouse=${DBT_WAREHOUSE}&role=${DBT_ROLE}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      retries: 5
      start_period: 10s
      timeout: 10s

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./dockerfiles/Dockerfile
      args:
        AIRFLOW_UID: ${LOCAL_UID:-1000}
        AIRFLOW_GID: ${LOCAL_GID:-0}
    user: "${LOCAL_UID:-1000}:0"
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW_UID=${LOCAL_UID:-1000}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: scheduler

  airflow-worker:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile-worker
      args:
        AIRFLOW_UID: ${LOCAL_UID:-1000}
        AIRFLOW_GID: ${LOCAL_GID:-0}
    user: "${LOCAL_UID:-1000}:0"
    container_name: airflow_worker
    restart: always
    depends_on:
      - airflow-webserver
      - airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - DBT_PROFILES_DIR=/opt/dbt/config
      - AIRFLOW_UID=${LOCAL_UID:-1000}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./dbt:/opt/airflow/dbt
      - ./dbt/profiles.yml:/opt/dbt/config/profiles.yml
    command: celery worker
volumes:

  postgres_data:
    driver: local